{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed483e4",
   "metadata": {
    "id": "i4aLXKwOz73V"
   },
   "source": [
    "# Recommandation de films via la factorisation matricielle\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Dans cet atelier, nous proposons d'implémenter un système de recommandation basé sur la factorisation matricielle. Nous utiliserons la base de données <a href=\"https://grouplens.org/datasets/movielens/\">MovieLens</a> afin d'entraîner nos modèles, mener certaines expériences et comparer nos résultats avec d'autres types d'architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993157e",
   "metadata": {
    "id": "XDKq8-yXyQZ1"
   },
   "source": [
    "\n",
    "<!-- #region id=\"pIG0GWqkz8BC\" -->\n",
    "\n",
    "## 1.1 Installation des librairies\n",
    "\n",
    "Avant de commencer, nous devons nous assurer d'installer les librairies nécessaires pour le tutoriel à l'aide de pip. Pour ce faire, exécutez la cellule suivante en la sélectionnant et en cliquant `shift`+`Enter`. Ceci peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b86c98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "and193IDyQZ2",
    "outputId": "d28084dc-4660-4d24-fb72-902d66eaa737"
   },
   "outputs": [],
   "source": [
    "!rm -rf RS-Workshop\n",
    "!git clone https://github.com/davidberger2785/RS-Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a6642",
   "metadata": {
    "id": "jXnUNiZPyQZ8"
   },
   "source": [
    "Afin de vous assurer que l'installation ait eu lieu, importez toutes les libraries et modules dont nous nous servirons pour cet atelier en exécutant la prochaine cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b8a9f",
   "metadata": {
    "id": "ffIsFyBmyQZ8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation des données\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Fonctions maison\n",
    "sys.path += ['RS-Workshop/Tutoriels - En/']\n",
    "import utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c3579",
   "metadata": {
    "id": "pjO04i-MyQZ_"
   },
   "source": [
    "Nous avons également écrit quelques fonctions passe-partout que nous avons regroupées dans la librairie utilities. En fait, ces différentes fonctions existent fort probablement déjà en python, mais nous en ignorons simplement l'existence..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e1137",
   "metadata": {
    "id": "bt2C99HCyQZ_"
   },
   "source": [
    "## 1.2 Objectif\n",
    "\n",
    "De façon générale, l'objectif d'un système de recommandation est, comme son nom l'indique, d'effectuer des recommandations personnalisées à chacun des utilisateurs. Idéalement, ces recommandations devront être bonnes, bien que ce concept puisse rapidement devenir flou. Contrairement à d'autres tâches en apprentissage automatique, telle la reconnaissance d'images de chats ou la prédiction du cours d'une action en bourse, effectuer des recommandations de manière à aider un utilisateur est d'autant plus complexe que ce problème est plus ou moins bien défini. Cherchons-nous à présenter à un utilisateur précis des suggestions le confortant dans ses choix antérieurs? Ou enconre, voulons-nous lui présenter des suggestions complémentaires ou totalement indépendantes des items précédemment considérés? Enfin, tenterons-nous plutôt de lui présenter des items auxquels il n'a pas encore été exposé? Chacune des ces options sont légitimes et pourront être modélisées. Sans perte de généralités, le schéma ci-dessous modélise simplement la problématique des systèmes de recommandation sous l'angle de l'apprentissage automatique.\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/High_level_1.png?raw=1)\n",
    "\n",
    "\n",
    "\n",
    "N'empêche, dans le cadre de cet atelier et en considérant le contexte dans lequel nous sommes plongés, soit la suggestion de films comme le font Netflix ou Amazon Prime, nous pouvons réduire le problème à une tâche relativement simple: recommander des films que l'utilisateur va aimer en fonction de ses intérêts passés. Afin de mener à bien cette tâche, nous utiliserons l'ensemble des préférences des usagers, certaines variables sociodémographiques associées de même que certaines caractéristiques des films. Enfin, nous pouvons rafiner le schéma ainsi:\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/High_level_2.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed5d84",
   "metadata": {
    "id": "Z8kkOcZLyQaA"
   },
   "source": [
    "## 1.3 Jeu de données - MoviesLens 100k\n",
    "\n",
    "Les données utilisées consistent ici en plus ou moins 100 000 évaluations de films effectuées par 943 utilisateurs et où un ensemble de 1 682 films étaient disponibles en visionnement. En plus des 100 000 évaluations à notre disposition, nous avons des informations complémentaires liées à chacun des usagers de même qu'à chacun des films.\n",
    "\n",
    "En somme, nous allons utiliser un total de trois jeux de données afin de mener à bien nos analyses soit:\n",
    "\n",
    "* Users : contenant des informations associées aux caractéristiques des utilisateurs,\n",
    "* Movies : contenant des informations associées aux caractéristiques des films en visionnement,\n",
    "* Ratings : contenant l'ensemble des 100 000 évaluations effectuées par les utilisateurs.\n",
    "\n",
    "Nous utiliserons la librairie <a href=\"https://pandas.pydata.org/\">Pandas</a> pour télécharger la base de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690b913",
   "metadata": {
    "id": "apGD8Z_CyQaA"
   },
   "source": [
    "### 1.3.1 Users: Importation et prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efcd8c",
   "metadata": {
    "id": "F6pLOxHjyQaB"
   },
   "outputs": [],
   "source": [
    "# Téléchargement des données\n",
    "ROOT_DIR = 'RS-Workshop/'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/ml-100k/')\n",
    "\n",
    "users = pd.read_csv(os.path.join(DATA_DIR, 'u.user'), \n",
    "                        sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Nous définissons les différentes variables en fonction de l'information fournie dans le fichier 'readme'\n",
    "users.columns = ['Index', 'Age', 'Gender', 'Occupation', 'Zip code']\n",
    "\n",
    "# Bref aperçu\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c8b8a",
   "metadata": {
    "id": "6xg1jIhbyQaD"
   },
   "source": [
    "Avant de présenter les statistiques descriptives liées à la population étudiée, nous allons dans un premier temps traiter les données associées aux usagers sous la forme d'une <a href=\"https://en.wikipedia.org/wiki/List_(abstract_data_type)\">list</a> afin de pouvoir plus aisément les manipuler. Notons que l'occupation de chaque individu étant une chaîne de caractères, nous avons recodé les 21 occupations possibles en booléen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c05b81",
   "metadata": {
    "id": "C7KCPwC9yQaE"
   },
   "outputs": [],
   "source": [
    "# Nombre d'utilisateurs et d'utilisatrices\n",
    "nb_users = len(users)\n",
    "\n",
    "# Sexe\n",
    "gender = np.where(np.matrix(users['Gender']) == 'M', 0, 1)[0]\n",
    "\n",
    "# Occupation\n",
    "occupation_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.occupation'), \n",
    "                                            sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Recodage en booléen de la variable occupation\n",
    "occupation_matrix = np.zeros((nb_users, len(occupation_name)))\n",
    "\n",
    "for k in np.arange(nb_users):\n",
    "    occupation_matrix[k, occupation_name.tolist().index(users['Occupation'][k])] = 1\n",
    "\n",
    "# Concatenation des différentes données sociodémographiques sous forme de liste\n",
    "user_attributes = np.concatenate((np.matrix(users['Age']), np.matrix(gender), occupation_matrix.T)).T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af58842",
   "metadata": {
    "id": "MupJaXiZyQaG"
   },
   "source": [
    "Nous explorons par la suite les différentes statistiques descriptives associées aux usagers. Celles-ci comportent des informations en lien avec l'âge (variable continue), le sexe (variable binaire) et l'occupation de chacun des usagers (au nombre de 21, toutes binaires).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7bd0",
   "metadata": {
    "id": "cMm2LDjSyQaG"
   },
   "source": [
    "Statistiques descriptives associées à <i>Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80be45",
   "metadata": {
    "id": "ZF4Y5TUQyQaH"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users['Age'].describe()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9652af",
   "metadata": {
    "id": "LeX_TqtCyQaJ"
   },
   "source": [
    "Diagramme à bandes pour la stastistique associée au <i> genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae1461",
   "metadata": {
    "id": "ktjctlNPyQaJ"
   },
   "outputs": [],
   "source": [
    "utl.barplot(['Women', 'Men'], np.array([np.mean(gender) , 1 - np.mean(gender)]) * 100, \n",
    "            'Sex', 'Percentage (%)', \"User's gender\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f84258",
   "metadata": {
    "id": "Tv5sHwVZyQaO"
   },
   "source": [
    "Diagramme à bandes pour la stastistique associée à <i>Occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5694d",
   "metadata": {
    "id": "z0_b6pd9yQaP"
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(occupation_name, np.mean(occupation_matrix, axis=0) * 100)\n",
    "utl.barplot(attributes, scores, 'Occupation', 'Percentage (%)', \"User's occupation\", 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e539d36",
   "metadata": {
    "id": "PyIfNODfyQaR"
   },
   "source": [
    "### 1.3.2 Movies: Importation et reformatage des données\n",
    "\n",
    "De la même façon, nous allons traiter et explorer les données associées aux films. Pour chacun d'eux, nous disposons du titre, de la date de sortie en Amérique du Nord, de même que les genres auxquels il est associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309ee48",
   "metadata": {
    "id": "Em0nC2FpyQaR"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(DATA_DIR, 'u.item'), sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Nombre de films\n",
    "nb_movies = len(movies)\n",
    "\n",
    "# Genres\n",
    "movies_genre = np.matrix(movies.loc[:, 5:])\n",
    "movies_genre_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.genre'), sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Survol rapide\n",
    "movies.columns = ['Index', 'Title', 'Release', 'The Not a Number column', 'Imdb'] + movies_genre_name.tolist()\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46580d0",
   "metadata": {
    "id": "c0uDnLOYyQaT"
   },
   "source": [
    "Nous présentons les proportions de films en fonction du genre comme statistique descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f36e2",
   "metadata": {
    "id": "eqaRcdBtyQaU"
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(movies_genre_name, \n",
    "                                   np.array(np.round(np.mean(movies_genre, axis=0) * 1, 2))[0])\n",
    "utl.barplot(attributes, np.array(scores) * 100, xlabel='Genre', ylabel='Percentage (%)', \n",
    "            title=\" \", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f362ef6",
   "metadata": {
    "id": "8BIFt6pxyQaW"
   },
   "source": [
    "### 1.3.3 Ratings: Importation et traitement des données\n",
    "\n",
    "La base de données comportant les évaluations des films effectuées par les usagers est constituée d'environ 100 mille lignes (une évaluation par ligne) où sont respectivement recensés le numéro d'identification de l'utilisateur, le numéro d'identification du film, l'évaluation associée et un marqueur de temps auquel le film a été visionné. Les ensembles d'entraînement et de test ont été fournis tels quels, c'est-à-dire que nous n'avons pas besoin de les construire nous-même, et comportent respectivement 80 et 20 mille évaluations.\n",
    "\n",
    "Pour des raisons pratiques, nous convertissons la base de données sous la forme d'une liste grâce à la fonction maison convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f21184",
   "metadata": {
    "id": "SlMXfiltyQaX"
   },
   "outputs": [],
   "source": [
    "training_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.base'), delimiter='\\t'), dtype='int')\n",
    "testing_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.test'), delimiter='\\t'), dtype='int')\n",
    "\n",
    "train_set = utl.convert(training_set, nb_users, nb_movies)\n",
    "test_set = utl.convert(testing_set, nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f67796",
   "metadata": {
    "id": "5zvO5UCTyQaZ"
   },
   "source": [
    "Comme nous l'avons fait auparavant, nous pouvons obtenir quelques statistiques descriptives associées aux évaluations. Dans une premier temps, il pourrait être intéressant d'étudier les tendances moyennes des individus.\n",
    "\n",
    "##### Question 1\n",
    "\n",
    "1. Quelles autres types de statistiques pourraient être intéressantes?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "1. Nous pourrions définir les statistiques ci-dessous selon d'autres attributs, tels que le sexe des utilisateurs, leur profession ou leur âge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee8615",
   "metadata": {
    "id": "qu-AEw28yQaZ"
   },
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_set)\n",
    "shape = (len(train_set), len(train_set[0]))\n",
    "train_matrix.reshape(shape)\n",
    "train_matrix_bool = np.where(train_matrix > 0 , 1, 0)\n",
    "\n",
    "user_watch = np.sum(train_matrix_bool, axis=1)\n",
    "pd.DataFrame(user_watch).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f258f7",
   "metadata": {
    "id": "T9Dsm_LZyQac"
   },
   "source": [
    "Avec un petit histogramme..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c03d1",
   "metadata": {
    "id": "XQhKtRRFyQad"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.title('Empirical distribution of \\n the number of movies watched per user')\n",
    "plt.xlabel('Number of movies watched')\n",
    "plt.ylabel('Number of users')\n",
    "plt.hist(user_watch, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a4664",
   "metadata": {
    "id": "NJNTBhdeyQag"
   },
   "source": [
    "Nous présentons finalement quelques statistiques associées aux films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f017b",
   "metadata": {
    "id": "vZXYJ4eryQah"
   },
   "outputs": [],
   "source": [
    "movie_frequency = np.mean(train_matrix_bool, axis=0)\n",
    "pd.DataFrame(movie_frequency).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1010fd",
   "metadata": {
    "id": "67pC2NGKyQaj"
   },
   "source": [
    "##### Question 2\n",
    "\n",
    "a. Quelles statistiques ou observations pourraient nous paraître pertinentes? Pourquoi?\n",
    "\n",
    "b. Quel type de statistique pourrait être plus approprié dans un tel contexte?\n",
    "\n",
    "\n",
    "1. Un aperçu à l'histogramme nous informe qu'environ 25$\\%$ des films ont été visionnés par moins de 1$\\%$ de la population. En d'autres termes, certains films ont été visionnés par une large majorité. En fait, il est déjà assez difficile de définir une tendance, car il existe des événements rares, tels que des blockbusters (Toys Story) et une grande quantité de valeurs pratiquement nulles. Dans tous les cas, les statistiques les moins pertinentes pour nous sont la moyenne et l'erreur standard.\n",
    "2. Toute statistique descriptive robuste aux événements rares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28efe79",
   "metadata": {
    "id": "Z4KxdnsTyQaj"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Proportion of the population who watched the movie')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.hist(movie_frequency, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb873a1d",
   "metadata": {
    "id": "Qfpcfvk_yQal"
   },
   "source": [
    "#### Quel type de statistique pourrait être plus approprié dans un tel contexte?\n",
    "\n",
    "Nous pourrions également nous intéresser au comportement d'un individu en particulier. Entre autres choses, nous pourrions étudier s'il y a un biais associé à son schème d'évaluation ou encore quelles sont ses préférences cinématographiques en fonction du score attribué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cb310",
   "metadata": {
    "id": "74RuNFKtyQal"
   },
   "outputs": [],
   "source": [
    "def stats_user(data, movies_genre, user_id):\n",
    "    \n",
    "    ratings = data[user_id]\n",
    "    stats = np.zeros(6)\n",
    "    eva = np.zeros((6, movies_genre.shape[1]))\n",
    "\n",
    "    for k in np.arange(len(ratings)):\n",
    "        index = int(ratings[k])\n",
    "        stats[index] += 1\n",
    "        eva[index, :] = eva[index, :] + movies_genre[k]\n",
    "\n",
    "    return stats, eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da04dd",
   "metadata": {
    "id": "J9sH_M2HyQan"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "stats, eva = stats_user(train_set, movies_genre, user_id)\n",
    "utl.barplot(np.arange(5) + 1, stats[1:6] / sum(stats[1:6]), xlabel='Number of stars', ylabel='Percentage of movies (%)', \n",
    "            title=\" \", rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb68dbc",
   "metadata": {
    "id": "zuIGfhUFyQaq"
   },
   "source": [
    "##### Question 3\n",
    "\n",
    "3. Comment vérifier qu'il existe un biais associé au schème d'évaluation d'un individu?\n",
    "\n",
    "**Réponse 3**\n",
    "\n",
    "1. Un moyen simple serait d'effectuer un test t afin de vérifier si le score moyen d'un individu est significativement différent de ceux de l'ensemble de la population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fe004",
   "metadata": {
    "id": "s4jcmvzwyQaq"
   },
   "source": [
    "## 1.4 Création des sous-ensembles d'entraînement et de validation\n",
    "\n",
    "En apprentissage automatique, nous manipulons des <a href=\"https://blogs.nvidia.com/blog/2018/04/15/nvidia-research-image-translation/\">bases de données complexes</a> pour lesquelles nous tentons de définir des espaces de fonctions tout aussi complexes dans le but d'accomplir une tâche précise. Ceci étant, ces espaces de fonctions sont définis par un ensemble de paramètres dont le nombre tend à augmenter avec la complexité des données. Une fois l'espace défini par un ensemble de paramètres fixés, nous pouvons varier les différentes valeurs d'hyperparamètres afin d'explorer empiriquement les espaces de fonctions. Pour choisir l'ensemble des paramètres et d'hyperparamètres optimaux, nous définissons une métrique nous permettant d'évaluer le modèle; par exemple, à quel point l'image d'un chat nous paraît vraisemblable.\n",
    "\n",
    "Dans la mesure où nous voulons développer un modèle capable de généraliser, l'évaluation de ses performances, et donc la sélection des (hyperparamtètres) doit se faire sur un ensemble de données indépendant, mais issues de la même distribution, de l'ensemble sur lequel il a appris. Pareil ensemble porte le nom d'ensemble de validation.\n",
    "\n",
    "**! Remarque !** \n",
    "\n",
    "La notion d'ensemble d'entraînement et de test dans le cadre de système de recommandation est quelque peu différente de ce que l'on voit habituellement avec les problèmes dits supervisés. Si dans le cadre d'un problème supervisé, l'ensemble de test consiste essentiellement en de nouvelles observations (lire lignes d'un fichier) indépendantes des observations préalablement observées dans l'ensemble d'entraînement, le paradigme est sensiblement différent lorsque nous travaillons avec des systèmes de recommandation.\n",
    "\n",
    "Effectivement, et en raison du modèle mathématique sur lequel est basé les systèmes de recommandation, les données appartenant à l'ensemble de test ne sont pas liées à un nouvel individu, mais bien à de nouvelles évaluations, faites par le même ensemble d'individus, mais jusqu'alors inobservées. Dès lors, les données associées aux ensembles d'entraînement, de validation et de test ne sont plus indépendantes tel que supposé (la fameuse hypothèse iid ); ce qui complique théoriquement les choses.\n",
    "\n",
    "Puisque le but de l'atelier n'est pas d'étudier la notion de biais associée au type de dépendance entre les différents évaluations dans les systèmes de recommendation, nous allons naïvement supposer que chacune des évaluations sont indépendantes les unes des autres. N'empêche, dans un cadre pratique, ignorer ce genre de considérations pourra éventuellement biaiser les algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88181804",
   "metadata": {
    "id": "vvqL3zJmyQaq"
   },
   "outputs": [],
   "source": [
    "def split(data, ratio, tensor=False):\n",
    "    train = np.zeros((len(data), len(data[0]))).tolist()\n",
    "    valid = np.zeros((len(data), len(data[0]))).tolist()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] > 0:\n",
    "                if np.random.binomial(1, ratio, 1):\n",
    "                    train[i][j] = data[i][j]\n",
    "                else:\n",
    "                    valid[i][j] = data[i][j]\n",
    "\n",
    "    return [train, valid]\n",
    "\n",
    "train_0 = split(train_set, 0.8)\n",
    "test = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3355fc5",
   "metadata": {
    "id": "0tYJWzOVyQaw"
   },
   "source": [
    "# 2\\. Systèmes de recommandation : factorisation matricielle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c16103",
   "metadata": {
    "id": "sF4nUjipz8Bh",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2.1 Modèle\n",
    "\n",
    "La factorisation matricielle suppose que chaque évaluation observée $r\\_{ui}$ pour $1 \\\\leq u \\\\leq \\|U\\|$ et $1 \\\\leq i \\\\leq \\|I\\|$, dans lesquelles $\\|U\\|$ et $\\|I\\|$ représentent respectivement le nombre d’utilisateurs et le nombre de films, peut être estimée à l’aide d’un modèle latent. Ce modèle présente l’estimation $\\\\hat{r}_{ui}$ de l’évaluation observée $r_{ui}$ ainsi :\n",
    "\n",
    "$$ \\\\begin{align} \\\\hat{r}_{ui} =  \\\\langle p_{u}, q\\_{i} \\\\rangle, \\\\end{align} $$\n",
    "\n",
    "dans laquelle $\\\\langle \\\\cdot \\\\rangle$ est le produit scalaire et $p\\_{u}$ et $q\\_{i}$ sont les représentations latentes associées à l’utilisateur *u* et à l'item *i*. L’intuition derrière cette représentation suggère que chaque évaluation peut être estimée en considérant une caractérisation latente des utilisateurs et des items.\n",
    "\n",
    "Par exemple, déterminons qu’il y a 3 variables latentes et supposons qu’elles sont associées respectivement à la popularité des films selon le box-office, à leur durée et à leur degré de romance. Supposons que l’utilisateur *u* est un adolescent de 15 ans qui aime les films d’horreur populaires d’assez courte durée : $$ \\\\begin{align} p\\_{u} = \\[1, 0, 0]\\^T. \\\\end{align} $$\n",
    "\n",
    "Supposons maintenant que le film *i* s'avère être *Le Roi Lion* selon la modélisation latente suivante :\n",
    "\n",
    "$$ \\\\begin{align} q\\_{i} = \\[1, 0.5, 0]\\^T. \\\\end{align} $$\n",
    "\n",
    "L’estimation de l’évaluation pour cet utilisateur et cet élément en fonction des représentations latentes sera donc :\n",
    "\n",
    "$$ \\\\begin{align} \\\\hat{r}\\_{ui} =  \\\\langle p\\_u, q\\_i \\\\rangle = 1. \\\\end{align} $$\n",
    "\n",
    "Le défi principal pour ce type de modèle est de définir l’ensemble des vecteurs latents associés aux utilisateurs, regroupés dans la matrice $\\\\mathbf{P}_{\\|U\\| \\\\times k} = \\[p\\_1, p\\_2, .. ., p\\_k]$, et aux éléments, regroupés dans la matrice $ \\\\mathbf{Q}_{\\|I\\| \\\\times k} = \\[q\\_1, q\\_2, ..., q\\_k] $.   Puisque le problème initial est de présenter les estimations les plus exactes, donc de calculer $\\\\mathbf{P}$ et $\\\\mathbf{Q}$ de façon à minimiser les différences entre la totalité des évaluations observées $r\\_{ui}$ et leur estimation $\\\\hat {r}\\_{ui}$, nous pouvons définir la tâche à accomplir avec le problème d’optimisation suivant :\n",
    "\n",
    "$$ \\\\begin{align} \\\\mathbf{P}, \\\\mathbf{Q} = \\\\underset{p, q}{\\\\operatorname{argmin}} \\\\sum\\_{r\\_{ui} \\\\neq 0} (r\\_{ui} - \\\\hat{r}_{ui})\\^2 = \\\\underset{p, q}{\\\\operatorname{argmin}}  \\\\sum_{r\\_{ui} \\\\neq 0} (r\\_{ui} - \\\\langle p\\_u, q\\_i \\\\rangle)\\^2. \\\\end{align} $$\n",
    "\n",
    "Nous pouvons aussi régulariser les variables latentes dans le but de forcer les vecteurs associés à présenter des valeurs autres que 0 :\n",
    "\n",
    "$$ \\\\begin{align} \\\\mathbf{P}, \\\\mathbf{Q} = \\\\underset{p, q}{\\\\operatorname{argmin}} \\\\sum\\_{r\\_{ui} \\\\neq 0} {(r\\_{ui} - \\\\langle p\\_u, q\\_i \\\\rangle)\\^2 + \\\\lambda(\\|\\|p\\_u\\|\\|\\^2 + \\|\\|q\\_i\\|\\|\\^2)}, \\\\end{align} $$\n",
    "\n",
    "où $\\\\lambda$ est l’hyperparamètre de régularisation, soit la <i>pénalisation des poids</i> en apprentissage profond et le multiplicateur de Lagrange en mathématiques. Notez que les vecteurs latents avec très peu de valeurs nulles prédiront en retour des évaluations d’une valeur autre que 0. Puisque nous essayons de proposer de nouvelles recommandations, cette contrainte semble utile pour éviter une estimation sous la forme d’une matrice creuse $\\\\hat{\\\\mathbf{R}}$.\n",
    "\n",
    "En général, ce problème d’optimisation consistant à factoriser une matrice creuse ne peut pas être résolu aussi facilement qu’un problème de régression linéaire, entre autres, à l’aide de la méthode des moindres carrés. Dans ce tutoriel, nous introduirons l’algorithme de descente de gradient stochastique dans une approche visant à résoudre le problème d'optimisation pour estimer les matrices $\\\\mathbf{P}$ et $\\\\mathbf{Q}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65636369",
   "metadata": {
    "id": "-RkDWAtPz8Bj",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2.2 Implémentation\n",
    "\n",
    "Afin de concevoir un système de recommandation basé sur la factorisation matricielle, il faut déterminer quelques fonctions spécifiques requises pour ce type d’algorithme. En gros, l’implémentation se divise en trois composantes :\n",
    "\n",
    "1. **La boucle d’entraînement** : processus d’optimisation itératif permettant d’estimer à quel point le modèle n’arrive pas à satisfaire un objectif spécifique et de faire les adaptations nécessaires au modèle jusqu’à ce que le critère d’arrêt soit atteint.\n",
    "\n",
    "2. **La fonction de perte** : fonction calculant à quel point les prédictions du modèle diffèrent des observations réelles.\n",
    "\n",
    "3. **L’estimation** : processus d’estimation des matrices des facteurs $\\\\mathbf{P}$ et $\\\\mathbf{Q}$ associés respectivement avec les utilisateurs et les éléments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d1b30",
   "metadata": {
    "id": "uuSCo_qfPRFc",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2.2.1 Boucle d’entraînement\n",
    "\n",
    "Configurons maintenant la boucle d’entraînement. Pour cela, nous utilisons une fonction qui effectue un certain nombre d’itérations pour actualiser les paramètres de notre modèle jusqu’à ce que le critère d'arrêt soit atteint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa3de3",
   "metadata": {
    "id": "WZJrzBL_z8Bn",
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Question 4\n",
    "\n",
    "Supposons que nous avons le choix entre la fonction de prédiction, la descente de gradient stochastique et la fonction de perte. Nous sauterons l’implémentation de ces fonctions pour se concentrer sur leur signature (vous pouvez jeter un coup d’œil aux cellules de code ci-dessous).\n",
    "\n",
    "1. Dans le cas des estimateurs obtenus via la descente de gradient stochastique (SGD), quelle condition devrions-nous rajouter à la ligne 17? Et pourquoi?\n",
    "2. À la fin de chaque époque, quelle statistique serait-il préférable de calculer? Codez-la. Remarque : il est préférable d'initialiser des objets en début de fonction (voir ligne 6).\n",
    "3. Donnez deux raisons pour lesquelles ces statistiques sont utiles.\n",
    "4. Le critère d'arrêt pour éviter le surapprentissage à la ligne 30 est plutôt naïf. Pourquoi?\n",
    "5. Développez un nouveau critère d'arrêt.\n",
    "\n",
    "##### Réponse 4\n",
    "\n",
    "1. Tel que présenté dans la présentation du modèle, seules les évaluations observées, et donc les valeurs non nulles de la matrice $\\mathbf{R}$ sont considérées.\n",
    "2. En ce qui nous concerne, l'erreur quadratique moyenne sur les ensembles d'entraînement et de validation devront être calculées. De façon générale, après chaque époque, nous calculerons la fonction de perte sur les différents ensembles (mais pas celle associée à l'ensemble de test!). \n",
    "3. Sous l'hypothèse qu'une fonctiond de coût est monotone sur l'ensemble d'entraînement (bref que le modèle s'améliore constamment sur cet ensemble), il est fort probable que celui-ci va surapprend ce même ensemble. Afin de contrecarrer cette difficulté, un critère d'arrêt peut jouer le rôle de garde-fou contre le surapprentissage. Enfin, la deuxième raison est plutôt de nature computationnelle; il peut être intéressant que l'algorithme s'arrête après un certain temps et nous fournisse un modèle...\n",
    "4. Le critère ne considère qu'une seule valeur. Cette valeur nous semble un peu aléatoire; est-ce beaucoup ou est-ce trop peu considérant la vraisemblance des données que nous manipulons?\n",
    "5. Le nouveau critère prend en compte la dernière remarque. En fait, nous proposons de considérer la différence entre les deux dernières entrées associéed l'ensemble d'entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0d340",
   "metadata": {
    "id": "yhxDFNIrz8Bo"
   },
   "outputs": [],
   "source": [
    "def learn_to_recommend(data, features=10, lr=0.0002, epochs=101, weigth_decay=0.02, stopping=0.001):\n",
    "       \"\"\"\n",
    "    Args:\n",
    "      data: ensemble des évaluations\n",
    "      features: variables latentes\n",
    "      lr: learning rate pour la descente de gradient\n",
    "      epochs: nombre d'iterations ou boucles maximales à effectuer\n",
    "      weigth_decay: régularisation de type L2 afin de predire des valeurs differentes de 0\n",
    "      stopping: scalaire associé au critère d'arrête\n",
    "      \n",
    "    Returns:\n",
    "      P: matrice latente associée aux usagers\n",
    "      Q: matrice latente associée aux items\n",
    "      loss_train: un vecteur des différentes valeurs de la fonction perte après chaque itération sur train\n",
    "      loss_valid: un vecteur des différentes valeurs de la fonction perte après chaque itération pas sur valid\n",
    "      \"\"\"\n",
    "     \n",
    "    train, valid = data[0], data[1]\n",
    "    nb_users, nb_items = len(train), len(train[0])\n",
    "\n",
    "    # Réponse 4.2: Liste à initialiser \n",
    "    loss_train, loss_valid = [], []\n",
    "    \n",
    "    P = np.random.rand(nb_users, features) * 0.1\n",
    "    Q = np.random.rand(nb_items, features) * 0.1\n",
    "    \n",
    "    for k in range(epochs):        \n",
    "        for i in range(nb_users):\n",
    "            for j in range(nb_items):\n",
    "\n",
    "                # Réponse 4.1: codez la condition sur cette ligne\n",
    "                if train[i][j] > 0:\n",
    "                    eij = train[i][j] - prediction(P, Q, i, j)\n",
    "                    P, Q = sgd(eij, P, Q, i, j, features, lr)\n",
    "                               \n",
    "        # Questions 4.2: Codez la statistique\n",
    "        loss_train.append(loss(train, P, Q))\n",
    "        loss_valid.append(loss(valid, P, Q))\n",
    "        \n",
    "        if k % 10 == 0:\n",
    "            print('Epoch : ', \"{:3.0f}\".format(k+1), ' | Train :', \"{:3.3f}\".format(loss_train[-1]), \n",
    "                  ' | Valid :', \"{:3.3f}\".format(loss_valid[-1]))\n",
    "\n",
    "        # Question 4.4: \n",
    "        if abs(loss_train[-1]) < stopping:\n",
    "            break\n",
    "            \n",
    "        # Question 4.5: Nouveau critère d'arrêt\n",
    "        #if abs(loss_valid[-1] - loss_valid[-2]) < 0.001:\n",
    "        #    break\n",
    "                \n",
    "    return P, Q, loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744247bc",
   "metadata": {
    "id": "NgT88hU5z8Bu",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2.2.2 Fonction de coût\n",
    "\n",
    "La fonction de coût joue un rôle déterminant dans la construction d'un modèle prédictif. En effect, c'est cette même fonction de coût que nous essaierons de minimiser (ou maximiser c'est selon) en ajustant de façon itérative les valeurs des matrices latentes.\n",
    "\n",
    "Dans la mesure où l'on considère que les évaluations considérées varient entre 1 et 5, l'erreur quadratique moyenne (EQM) semble une première option intéressante. Formellement, dans le cadre d'un système de recommandation, nous définierons l'EQM ainsi :\n",
    "\n",
    "$$ \\\\begin{align} MSE(\\\\mathbf{R}, \\\\hat{\\\\mathbf{R}}) = \\\\frac{1}{n} \\\\sum\\_{r\\_{ui} \\\\neq 0} (r\\_{ui} - \\\\hat{r}\\_{ui})\\^2, \\\\end{align} $$\n",
    "\n",
    "où $\\\\mathbf{R}$ et $\\\\hat{\\\\mathbf{R}} $ sont respectivement les matrices des évaluations observées et prédites et *n* représente le nombre d'évaluations. De la même façon, $r_{ui}$ et $\\hat{r}_{ui}$ sont des scalaires associés respectivement à l'évaluation observée et l'évaluation estimée de l'usager $u$ pour l'item $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002ebd1",
   "metadata": {
    "id": "OAPyYa9QPDpU",
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Question 5\n",
    "\n",
    "1. Supposons que nous voulons prédire l'évaluation de l'individu <i>i</i> pour le film <i>j</i>, comment devrions-nous nous y prendre? Implémentez la fonction prédiction.\n",
    "2. Un détail important est manquant dans la fonction `loss` suivante. En quoi cette erreur est fondamentale? Corrigez-la. \n",
    "\n",
    "##### Réponse 5\n",
    "\n",
    "2. Il manque la condition sur les valeurs non nulles!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43118b93",
   "metadata": {
    "id": "Ft_CJd64z8Bu"
   },
   "outputs": [],
   "source": [
    "# TODO 5.1:\n",
    "def prediction(P, Q, u, i):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       P: matrice des usagers\n",
    "       Q: matrice des items\n",
    "       i: indice associé à l'usager i\n",
    "       j: indice associé à l'item j\n",
    "    \n",
    "    Returns:\n",
    "       pred: l'évaluation prédite de l'usager i pour l'item j\n",
    "       \"\"\"\n",
    "    pass\n",
    "\n",
    "def loss(data, P, Q):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: données\n",
    "       P: matrice des usagers\n",
    "       Q: matrice des items\n",
    "       \n",
    "    Returns:\n",
    "        EQM: la moyenne observée des erreurs au carré\n",
    "        \"\"\"\n",
    "        \n",
    "    errors_sum, nb_evaluations = 0., 0\n",
    "    nb_users, nb_items = len(data), len(data[0])\n",
    "\n",
    "    for u in range(nb_users):\n",
    "        for i in range(nb_items):\n",
    "        \n",
    "            # Question 5.2: Un détail important\n",
    "            if data[i][j] > 0:\n",
    "                errors_sum += pow(data[i][j] - prediction(P, Q, i, j), 2)\n",
    "                nb_evaluations += 1\n",
    "                \n",
    "    return errors_sum / nb_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e3558",
   "metadata": {
    "id": "10bI2dV5z8CQ",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2.2.3 Estimation\n",
    "\n",
    "La méthode d'estimation des paramètres du modèle est directement associée à la fonction de coût que nous essayons de minimiser. Avec la factorisation matricielle, deux techniques d'estimation sont disponibles afin de calculer les matrices latentes $\\mathbf{P}$ et $\\mathbf{Q}$ respectivement associées aux usagers et aux items. Dans tous les cas, ces techniques font appel à la linéarité du modèle de factorisation matriciel.\n",
    "\n",
    "#### Descente de gradient\n",
    "\n",
    "Dans un premier temps, nous implémentons la descente stochastique du gradient (SGD): une méthode itérative passant en revue l'ensemble des évaluations non nulles pour chacun des usagers.\n",
    "\n",
    "Formellement, et en se rappelant que la fonction que nous essayons de minimiser est:\n",
    "\n",
    "$$ \\\\begin{align} \\\\underset{p, q}{\\\\operatorname{min}} L(\\\\mathbf{R}, \\\\lambda) = \\\\underset{p, q}{\\\\operatorname{min}} \\\\sum\\_{r\\_{ui} \\\\neq 0} {(r\\_{ui} - \\\\langle p\\_u, q\\_i \\\\rangle)\\^2 + \\\\lambda \\\\cdot (\\|\\|p\\_u\\|\\|\\^2 + \\|\\|q\\_i\\|\\|\\^2)}, \\\\end{align} $$\n",
    "\n",
    "nous calculons que les gradients de la précédente équation en fonction de $p_u$ et $q_i$ sont:\n",
    "\n",
    "$$ \\\\nabla\\_{p\\_{u}} L(\\\\mathbf{R}, \\\\lambda) =  -2q\\_{i} \\\\cdot \\\\epsilon\\_{ui} + 2\\\\lambda \\\\cdot p\\_{u} \\\\quad \\\\text{and} \\\\quad \\\\nabla\\_{q\\_{i}} L(\\\\mathbf{R}, \\\\lambda) =  -2p\\_{u} \\\\cdot \\\\epsilon\\_{ui} + 2\\\\lambda \\\\cdot q\\_{i}, $$\n",
    "\n",
    "où\n",
    "\n",
    "$$ \\\\epsilon\\_{ui} = r\\_{ui} - \\\\hat{r}\\_{ui}. $$\n",
    "\n",
    "Enfin, pour chaque itération, et dans la mesure où l'évaluation observée est non-nulle, chacune des mises à jours des vecteurs latentes pourra se faire ainsi:\n",
    "\n",
    "$$\n",
    "p_{u}^{(t+1)} \\leftarrow p_{u}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot q_{i}^{(t)} - \\lambda \\cdot p_{u}^{(t)}) \\\\\n",
    "q_{i}^{(t+1)} \\leftarrow q_{i}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot p_{u}^{(t)} - \\lambda \\cdot q_{i}^{(t)}),\n",
    "$$\n",
    "\n",
    "où $p_{u}^{(t+1)}$ dénote la valeur de $p_{u}$ après la $t + 1$ ième itération et où $\\gamma$ est le pas d'apprentissage (<i>learning rate</i>) de la descente. \n",
    "\n",
    "#### Remarque sur les moindres carrés alternés\n",
    "\n",
    "La deuxième technique est basée sur les moindres carrés alternés (ALS). Cette méthode a ceci d'élégant qu'elle permet une forme analytique. Nous ne l'implémenterons pas dans le cadre de cet atelier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed093138",
   "metadata": {
    "id": "8R7uiumPPLaq",
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Question 6\n",
    "\n",
    "1. Compte tenu des équations ci-dessus, pouvez-vous compléter la fonction `sgd` ci-dessous afin de mettre à jour les paramètres $\\\\mathbf{P}$ et $\\\\mathbf{Q}$ de notre modèle?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538bf53",
   "metadata": {
    "id": "8ic_KU6uz8Ca"
   },
   "outputs": [],
   "source": [
    "def sgd(error, P, Q, id_user, id_item, features, weight_decay, lr):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       error: différence entre l'évaluation observée et celle prédite (dans cet ordre)\n",
    "       P: matrix of users\n",
    "       Q: matrix of items\n",
    "       id_user: id_user\n",
    "       id_item: id_item\n",
    "       features: nombre de variables latente\n",
    "       lr: pas d'apprentissage pour la descente du gradient\n",
    "       \n",
    "    Returns:\n",
    "        P: la nouvelle estimation de P\n",
    "        Q: la nouvelle estimation de Q\n",
    "     \"\"\"\n",
    "    \n",
    "    # Question 6.1 : Implémentez la SGD\n",
    "    for l in range(features):\n",
    "        P[id_user, l] = P[id_user, l] + lr * (error * Q[id_item, l] - weigth_decay * P[id_user, l])\n",
    "        Q[id_item, l] = Q[id_item, l] + lr * (error) * P[id_user, l] - weigth_decay * Q[id_item, l]\n",
    "    \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b2571",
   "metadata": {
    "id": "7IwjGI_Bz8Ce",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2.3 Entraînement du modèle\n",
    "\n",
    "La factorisation matricielle maintenant implémentée, nous pouvons commencer à entraîner le modèle avec différents paramètres et hyperparamètres. L'idée ici n'est pas d'ajuster les paramètres de façon telle à obtenir le meilleur modèle possible, mais simplement de comprendre le rôle que ceux-ci peuvent jouer, tant du point de vue du surappentissage que du temps de calcul. En fait, ici, il n'y a que très peu de mauvaises réponses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2214778",
   "metadata": {
    "id": "OOMVmv6-z8DQ"
   },
   "outputs": [],
   "source": [
    "features = 5\n",
    "lr = 0.02\n",
    "epochs = 101\n",
    "weigth_decay = 0.02\n",
    "stopping = 0.01\n",
    "\n",
    "P, Q, loss_train, loss_valid = learn_to_recommend(train, features, lr, epochs, weigth_decay, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1f90c",
   "metadata": {
    "id": "XAs0VXIoz8DS",
    "lines_to_next_cell": 0
   },
   "source": [
    "Une fois le modèle entraîné, nous pouvons visualiser les différentes courbes d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2f7b0",
   "metadata": {
    "id": "qGJBjTy9z8DT"
   },
   "outputs": [],
   "source": [
    "x = list(range(len(loss_train)))\n",
    "k=0\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.plot(x[-k:], loss_train[-k:], 'r', label=\"Train\")\n",
    "plt.plot(x[-k:], loss_valid[-k:], 'g', label=\"Validation\")\n",
    "plt.title('Learning curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "leg = plt.legend(loc='best', shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18559787",
   "metadata": {
    "id": "ZdVEiUmyz8DV",
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Question 7\n",
    "\n",
    "1. Était-ce vraiment nécessaire de calculer autant d'époques?\n",
    "2. En nous inspirant de ces courbes, quel critère d'arrêt pourrions-nous développer?\n",
    "3. En quoi est-il plus pertinent que celui défini dans la boucle d'apprentissage?\n",
    "4. Implémentez-le.\n",
    "\n",
    "##### Réponse 7\n",
    "\n",
    "1. En considérant le comportement de la courbe de validation, qui à trois décimale près, ne change plus, visiblement non. \n",
    "2. Nous pourrions considérer un critère où l'arrêt d'une amélioration sur l'ensemble de validation provoque un arrêt automatique.\n",
    "3. Dans la mesure où les modèles en apprentissage automatique cherche à généraliser, un critère fonction d'un ensemble indépendant à l'ensemble d'entraînement est pertinent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde30c2e",
   "metadata": {
    "id": "zuprf-ouySLH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Réponse 7.1\n",
    "loss(test, P, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e88fd0",
   "metadata": {
    "id": "N7hI0Mzqz8DV",
    "lines_to_next_cell": 0
   },
   "source": [
    "Enfin, nous pouvons évaluer les performances de notre modèle sur l'ensemble test.\n",
    "\n",
    "##### Question 8\n",
    "\n",
    "1. Implémentez la procédure.\n",
    "2. En quoi est-ce pertinent d'évaluer les performances sur un tel ensemble?\n",
    "\n",
    "##### Réponse 8\n",
    "\n",
    "2. Les résultats sur l'ensemble de test sont utilisés comme étalon-or en apprentissage automatique. En fait, dans la littérature notamment, ils permettent de mesurer la capacité des modèles à généraliser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96f019",
   "metadata": {
    "id": "QDqJxtywz8DW"
   },
   "outputs": [],
   "source": [
    "# TODO 8.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64a4e1",
   "metadata": {
    "id": "VY2re70Nz8DZ",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2.4 Analyses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa85d6",
   "metadata": {
    "id": "H9vsE0-Iz8Da",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 2.4.1 Exploration des couches latentes\n",
    "\n",
    "Grâce à la factorisation matricielle, il est possible d'explorer les différentes variables latentes associées aux usagers et aux items. De par la nature des matrices $\\mathbf{P}$ et $\\mathbf{Q}$, l'exploration des <i>k</i> variables latentes associées aux colonnes de $\\mathbf{P}$ et $\\mathbf{Q}$ pourraient s'avérer intéressante. \n",
    "\n",
    "À titre d'exemple, supposons que les deux premières variables latentes de la matrice des objets $\\mathbf{Q}$ présentent les valeurs suivantes: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_1 &= [-1.0, \\ -0.8, \\ 0.0, \\ ..., \\ 1.0, \\ 0.5 ] \n",
    "\\qquad \\text{et} \\qquad\n",
    "q_2 = [-1.0, \\ 0.8,  \\ 1.0, \\ ..., \\ 0.5, \\ -0.8 ].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Enfin, supposons qu'à ces valeurs correspondent les films suivant: \n",
    "\n",
    "1. The Room (2003),\n",
    "2. Star Wars: Attack of the clones (2002),\n",
    "3. Titanic (1997),\n",
    "4. Citizen Kane (1954),\n",
    "5. The Nigthmare before Christmass (1993).\n",
    "\n",
    "En cartographiant ces films en fonctions des valeurs associées des deux premiers variables latentes, nous obtenons le graphique suivant:\n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/91663551-9486d300-eab7-11ea-8e9f-c58398eff9fe.png\" width = \"500\">\n",
    "\n",
    "Ce graphe permet possiblement de se faire une id&eacute;e des sch&eacute;mas sous-tendant chaque variable latente. Dans ce cas, on pourrait s&rsquo;imaginer que la premi&egrave;re variable latente est associ&eacute;e &agrave; comment le film est re&ccedil;u et que la deuxi&egrave;me est reli&eacute;e &agrave; la pr&eacute;sence d&rsquo;une grande vedette. Du moins, c&rsquo;est une hypoth&egrave;se int&eacute;ressante! Voyons s&rsquo;il y a des tendances similaires lorsqu&rsquo;on s&rsquo;int&eacute;resse aux valeurs associ&eacute;es &agrave; la matrice des utilisateurs $ \\mathbf{P}$. Supposez que les deux premi&egrave;res variables latentes de la matrice utilisateur $\\mathbf{P}$ prennent les valeurs suivantes&nbsp;: \n",
    "\n",
    "$$ \\begin{align} p_1 &amp;= [1.0, \\ 0.0, \\ -0.5, \\ 1.0, \\ -1.0, \\ ...] \\qquad \\text{and} \\qquad p_2 = [1.0, \\ 0.0, \\ 0.5, \\ -1.0, \\ -0.8, \\ ...] \\end{align} $$ \n",
    "\n",
    "Et supposez que chaque valeur dans ces deux ensembles correspond aux utilisateurs ci-dessous&nbsp;: \n",
    "\n",
    "1. Serena \n",
    "2. Kali \n",
    "3. Neil \n",
    "4. Mary \n",
    "5. David \n",
    "\n",
    "Relions maintenant les utilisateurs aux valeurs associ&eacute;es aux vecteurs $ p_1 $ et $ p_2 $. Remarquez que dans ce cas, nous avons consid&eacute;r&eacute; les deux m&ecirc;mes facteurs latents afin de pouvoir les comparer &agrave; la caract&eacute;risation des axes obtenus ant&eacute;rieurement&nbsp;: \n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/91663557-9e103b00-eab7-11ea-9ee1-0de6a5ac3760.png\" width = \"500\">\n",
    "\n",
    "Cette approche pourrait nous permettre de recommander de nouveaux films n&apos;ayant jamais &eacute;t&eacute; &eacute;valu&eacute;s par les utilisateurs simplement en se fiant &agrave; certaines caract&eacute;ristiques. Par exemple, il y a de grandes chances que Serena aime le prochain film de Scorsese <i>The Irishman</i> et que Neil ait h&acirc;te de visionner le nouveau film <i>Cats</i>. Nous proposons maintenant une fonction facilitant l&apos;exploration des variables latentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b43d6",
   "metadata": {
    "id": "yDEWyWn_z8Db"
   },
   "outputs": [],
   "source": [
    "def exploration(movie_titles, latent_matrix, frequency_mask, factor_idx, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       object_name: vecteur des noms des films\n",
    "       matrix: matrice latente associée aux films\n",
    "       freq: seuil de fréquence de visionnement minimal au-delà duquel nous pouvons considérer le film dans l'analyse\n",
    "       factor: le numéro de la variable latente que l'on veut étudier\n",
    "       k: nombre de films en sortie = 3*k - 1\n",
    "       \n",
    "    Returns:\n",
    "        names: le titre des films\n",
    "        scores: l'évaluation prédite associée\n",
    "    \"\"\"\n",
    "\n",
    "    # slice the column to obtain latent variable, then apply mask\n",
    "    latent_variable = latent_matrix[:, factor] * frequency_mask\n",
    "\n",
    "    # filter out infrequent movies\n",
    "    nonzero_indices = np.nonzero(latent_variable)\n",
    "    movies = np.array(movie_titles)[nonzero_indices][:k]\n",
    "    latent_variable = latent_variable[nonzero_indices][:k]\n",
    "\n",
    "    return movies, latent_variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba808caa",
   "metadata": {
    "id": "S2NyTbVWySLP",
    "lines_to_next_cell": 0
   },
   "source": [
    "Utilisons cette fonction et visualisons les résultats. Pour ce faire, nous considérerons les films ayant été visionnés par au moins 10 $\\%$ de tous les utilisateurs. Utilisons la liste movie\\_popularity créée plus tôt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d089354",
   "metadata": {
    "id": "8niveL1mySLQ"
   },
   "outputs": [],
   "source": [
    "# print(movie_popularity)\n",
    "# print(movie_popularity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ef5d6",
   "metadata": {
    "id": "nqSJGMbfz8Dg"
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "factor = 0\n",
    "threshold = 0.1\n",
    "names, scores = exploration(movies['Title'], Q, np.where(movie_frequency > threshold, 1, 0), factor, k)\n",
    "\n",
    "df = pd.DataFrame(np.matrix((names, scores)).T, (np.arange(len(scores)) + 1).tolist())\n",
    "df.columns = ['Title', 'Latent factor']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27742a31",
   "metadata": {
    "id": "nZuzfJHfz8Dg",
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Question 9\n",
    "\n",
    "1. Est-ce que certaines variables latentes peuvent être interprétables?\n",
    "2. Qu'arrivera-t-il si nous augmentons le nombre de variables latentes? Si nous le diminuons?\n",
    "\n",
    "##### Réponse 9\n",
    "\n",
    "1. Essentiellement, c'est ce qu'on souhaiterait. Cette idée est très bien présentée dans cet <a href=\"https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\"> article</a>. Par contre, ce n'est pas garanti et un semblant d'interprétation sera fonction du nombre de variables latentes et des données entre autres choses.\n",
    "2. Cela va dépendre. En fait, il faut bien comprendre que les modèles latents ne sont pas emboîtés les uns dans les autres. Par exemple, les <i>k</i> variables latentes d'un modèle ne vont pas contenir exactement la même information que les <i>k</i> premières variables latentes d'un modèle comportant un plus grand nombre de variables latentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ade72b",
   "metadata": {
    "id": "bNCN7hBpz8Di",
    "lines_to_next_cell": 0
   },
   "source": [
    "# 3\\. Applications\n",
    "\n",
    "L'un des objectifs premier des systèmes de recommandation est d'effectuer de recommandations (!) personnalisées pour chacun des utilisateurs. Dès lors, il pourrait être intéressant d'étudier les recommandations effectuées par notre modèle pour un individu spécifique. Naturellement, les recommendations faites ne suggèrent que des films non visionnés par l'usager.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d377f",
   "metadata": {
    "id": "vu1IfGByGB6c"
   },
   "source": [
    "##### Question 10\n",
    "\n",
    "1. Nous allons maintenant, pour un usager choisi, effectuer les dix meilleures recommandations associées. Pour ce faire, nous allons procéder par étapes simples tel que présentées dans le code. Notez que la fonction maison `rearrange` présentée ci-dessous pourrait vous être utile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337faec",
   "metadata": {
    "id": "7sFPWtOez8Dj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rank_top_k(names, ratings, k=10):\n",
    "   \"\"\"\n",
    "   Example:\n",
    "   a, b = np.array(['a', 'b', 'c']), np.array([6, 1, 3])\n",
    "   a, b = rank_top_k(a, b, k=2)\n",
    "   >>> a\n",
    "   np.array('a', 'c')\n",
    "   >>> b\n",
    "   np.array([6, 3])\n",
    "   \"\"\"\n",
    " \n",
    "   # rank indices in descending order\n",
    "   ranked_ids = np.argsort(ratings)[::-1]\n",
    " \n",
    "   return names[ranked_ids][:k], ratings[ranked_ids][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11ecc5",
   "metadata": {
    "id": "PI2exdGgz8Dm"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "top_k = 10\n",
    "\n",
    "user_id = 0\n",
    "top_what = 10\n",
    "\n",
    "# Étape 1: Définir quels films ont déjà été visionnés dans chacun des sous ensembles\n",
    "user_train, user_valid, user_test = np.array(train[0][user_id]), np.array(train[1][user_id]), np.array(test[user_id])\n",
    "user_train, user_valid, user_test = np.where(user_train > 0, 1, 0), np.where(user_valid > 0, 1, 0), np.where(user_test > 0, 1, 0)\n",
    "\n",
    "# Étape 2: Calculez l'ensemble de évaluations prédites pour l'individu choisi\n",
    "estimate = np.dot(P[user_id, :], Q.T)\n",
    "\n",
    "# Étape 3: Considérez seulement les évaluations associées aux ensembles d'entraînement et de validation\n",
    "estimate_train, estimate_valid = estimate * user_train, estimate * user_valid\n",
    "\n",
    "# Étape 4: Réorganisez les estimations sur les différents ensembles de manière à proposer les 10 meilleures\n",
    "#          recommandations\n",
    "recommendations, scores = rank_top_k(np.array(movies['Title']), estimate_valid)\n",
    "recommendations, scores = recommendations[-top_what:], scores[-top_what:]\n",
    "\n",
    "# Présentations des recommandations\n",
    "df = pd.DataFrame(np.matrix((recommendations, scores)).T, (np.arange(10) + 1).tolist())\n",
    "df.columns = ['Title', 'Predicted rating']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c5c64",
   "metadata": {
    "id": "16H76jlxz8Dq",
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "\n",
    "En manipulant rapidement le code ci-dessus, on s'aperçoit que les évaluations sur les prédictions sur les ensembles d'entraînement, de validation et même de test sont vraisemblables. Effectivement, ces dernières se situent entre 0 et 5, tout semble alors en ordre, ce qui nous paraît \"normal\" puisque les EQM sur les ensembles d'entraînement et de validation n'étaient pas particulièrement élevées.\n",
    "\n",
    "En fait, ça pourrait être intéressant de proposer à l'usager des films en fonction de ces préférences du moment ou en fonction du genre.\n",
    "\n",
    "##### Question 11\n",
    "\n",
    "1. Écrivez une fonction permettant d'effectuer pareille tâche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305fbe1",
   "metadata": {
    "id": "pS63d_flz8Dq"
   },
   "outputs": [],
   "source": [
    "def recommend(user_id, data, P, Q, list_of_genre_names, movies_genre, genre):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       user_id: user_id\n",
    "       data: ensemble des évaluations\n",
    "       P: matrice des invidus\n",
    "       Q: matrices des items\n",
    "       movies_genre: genre de film que l'utilisateur user_id veut écouter\n",
    "       new: Booléen, est-ce que nous voulons effectuer des nouvelles recommendations ou pas?\n",
    "    \n",
    "    Returns:\n",
    "        les meilleures suggestions en fonction du genre de film sélectionné\n",
    "    \"\"\" \n",
    "    \n",
    "    place = movies_genre_name.tolist().index(genre)    \n",
    "    genre = np.array(movies_genre[:, place])\n",
    "    predictions = np.array(np.dot(P[user_id, :], Q.T))\n",
    "    \n",
    "    #if new:\n",
    "    #    return np.array(predictions) * np.array(genre.T)[0] * np.where(data == 1, 0, 1)\n",
    "\n",
    "    return np.array(predictions) * np.array(genre.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba27d5",
   "metadata": {
    "id": "6TVDHmfoySLe"
   },
   "outputs": [],
   "source": [
    "# print(movies_genre_name)\n",
    "# print(movies_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbd41d",
   "metadata": {
    "id": "qVkOjfDhz8Du"
   },
   "outputs": [],
   "source": [
    "genre = \"Animation\"\n",
    "user_id = 1\n",
    "top_k = 5\n",
    " \n",
    "# Calcul et réorganisation des recommandations\n",
    "estimates = recommendations(user_id, train, P, Q, list_of_genre_names=movies_genre_name, movies_genre=movies_genre, genre=genre)\n",
    " \n",
    "recommendations, scores = rank_top_k(np.array(movies['Title']), estimates, k=top_k)\n",
    " \n",
    "# Presentation\n",
    "df = pd.DataFrame(np.matrix((recommendations, scores)).T, (np.arange(top_k) + 1).tolist(), columns = ['Title', 'Predicted rating'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
